{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import spatial\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "\n",
    "from graph_nets import graphs\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets.demos import models\n",
    "from graph_nets import data_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTANCE_WEIGHT_NAME = \"distance\"  # The name for the distance edge attribute.\n",
    "\n",
    "\n",
    "def pairwise(iterable):\n",
    "  \"\"\"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\"\"\n",
    "  a, b = itertools.tee(iterable)\n",
    "  next(b, None)\n",
    "  return zip(a, b)\n",
    "\n",
    "\n",
    "def set_diff(seq0, seq1):\n",
    "  \"\"\"Return the set difference between 2 sequences as a list.\"\"\"\n",
    "  return list(set(seq0) - set(seq1))\n",
    "\n",
    "\n",
    "def to_one_hot(indices, max_value, axis=-1):\n",
    "  one_hot = np.eye(max_value)[indices]\n",
    "  if axis not in (-1, one_hot.ndim):\n",
    "    one_hot = np.moveaxis(one_hot, -1, axis)\n",
    "  return one_hot\n",
    "\n",
    "\n",
    "def get_node_dict(graph, attr):\n",
    "  \"\"\"Return a `dict` of node:attribute pairs from a graph.\"\"\"\n",
    "  return {k: v[attr] for k, v in graph.node.items()}\n",
    "\n",
    "\n",
    "# Adapted from shortest_path demo\n",
    "class ShortestPathGraphDataset(data_util.GraphDataset):\n",
    "  def __init__(self, data_dir, min_nodes, max_nodes):\n",
    "    features_list = [\n",
    "      data_util.GraphFeature(key='input_graph',\n",
    "                             node_feature_size=5,\n",
    "                             edge_feature_size=1,\n",
    "                             global_feature_size=1,\n",
    "                             dtype='float32',\n",
    "                             description='Graph to input to network'),\n",
    "      data_util.GraphFeature(key='target_graph',\n",
    "                             node_feature_size=2,\n",
    "                             edge_feature_size=2,\n",
    "                             global_feature_size=1,\n",
    "                             dtype='float32',\n",
    "                             description='Graph to output from network'),\n",
    "      # Example of a non-graph feature\n",
    "      data_util.TensorFeature(\n",
    "          key='adj_mat_dense',\n",
    "          shape=[max_nodes, max_nodes],\n",
    "          dtype='float32',\n",
    "          description='Sparse adjacency matrix of input graph'),\n",
    "    ]\n",
    "    super(ShortestPathGraphDataset, self).__init__(data_dir, features_list)\n",
    "    self.min_nodes = min_nodes\n",
    "    self.max_nodes = max_nodes\n",
    "\n",
    "  def generate_graph(self,\n",
    "                     rand,\n",
    "                     num_nodes_min_max,\n",
    "                     dimensions=2,\n",
    "                     theta=1000.0,\n",
    "                     rate=1.0):\n",
    "    \"\"\"Creates a connected graph.\n",
    "\n",
    "    The graphs are geographic threshold graphs, but with added edges via a\n",
    "    minimum spanning tree algorithm, to ensure all nodes are connected.\n",
    "\n",
    "    Args:\n",
    "      rand: A random seed for the graph generator. Default= None.\n",
    "      num_nodes_min_max: A sequence [lower, upper) number of nodes per\n",
    "        graph.\n",
    "      dimensions: (optional) An `int` number of dimensions for the\n",
    "        positions. Default= 2.\n",
    "      theta: (optional) A `float` threshold parameters for the geographic\n",
    "        threshold graph's threshold. Large values (1000+) make mostly trees.\n",
    "        Try 20-60 for good non-trees. Default=1000.0.\n",
    "      rate: (optional) A rate parameter for the node weight exponential\n",
    "        sampling distribution. Default= 1.0.\n",
    "\n",
    "    Returns:\n",
    "      The graph.\n",
    "    \"\"\"\n",
    "    # Sample num_nodes.\n",
    "    num_nodes = rand.randint(*num_nodes_min_max)\n",
    "\n",
    "    # Create geographic threshold graph.\n",
    "    pos_array = rand.uniform(size=(num_nodes, dimensions))\n",
    "    pos = dict(enumerate(pos_array))\n",
    "    weight = dict(enumerate(rand.exponential(rate, size=num_nodes)))\n",
    "    geo_graph = nx.geographical_threshold_graph(num_nodes,\n",
    "                                                theta,\n",
    "                                                pos=pos,\n",
    "                                                weight=weight)\n",
    "\n",
    "    # Create minimum spanning tree across geo_graph's nodes.\n",
    "    distances = spatial.distance.squareform(\n",
    "        spatial.distance.pdist(pos_array))\n",
    "    i_, j_ = np.meshgrid(range(num_nodes), range(num_nodes), indexing=\"ij\")\n",
    "    weighted_edges = list(zip(i_.ravel(), j_.ravel(), distances.ravel()))\n",
    "    mst_graph = nx.Graph()\n",
    "    mst_graph.add_weighted_edges_from(weighted_edges,\n",
    "                                      weight=DISTANCE_WEIGHT_NAME)\n",
    "    mst_graph = nx.minimum_spanning_tree(mst_graph,\n",
    "                                         weight=DISTANCE_WEIGHT_NAME)\n",
    "    # Put geo_graph's node attributes into the mst_graph.\n",
    "    for i in mst_graph.nodes():\n",
    "        mst_graph.node[i].update(geo_graph.node[i])\n",
    "\n",
    "    # Compose the graphs.\n",
    "    combined_graph = nx.compose_all((mst_graph, geo_graph.copy()))\n",
    "    # Put all distance weights into edge attributes.\n",
    "    for i, j in combined_graph.edges():\n",
    "      combined_graph.get_edge_data(i, j).setdefault(DISTANCE_WEIGHT_NAME,\n",
    "                                                    distances[i, j])\n",
    "    return combined_graph, mst_graph, geo_graph\n",
    "\n",
    "  def add_shortest_path(self, rand, graph, min_length=1):\n",
    "    \"\"\"Samples a shortest path from A to B, adds attributes to indicate it.\n",
    "\n",
    "    Args:\n",
    "      rand: A random seed for the graph generator. Default= None.\n",
    "      graph: A `nx.Graph`.\n",
    "      min_length: (optional) An `int` minimum number of edges in the\n",
    "        shortest path. Default= 1.\n",
    "\n",
    "    Returns:\n",
    "      The `nx.DiGraph` with the shortest path added.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: All shortest paths are below the minimum length\n",
    "    \"\"\"\n",
    "    # Map from node pairs to the length of their shortest path.\n",
    "    pair_to_length_dict = {}\n",
    "    try:\n",
    "      # This is for compatibility with older networkx.\n",
    "      lengths = nx.all_pairs_shortest_path_length(graph).items()\n",
    "    except AttributeError:\n",
    "      # This is for compatibility with newer networkx.\n",
    "      lengths = list(nx.all_pairs_shortest_path_length(graph))\n",
    "    for x, yy in lengths:\n",
    "      for y, l in yy.items():\n",
    "        if l >= min_length:\n",
    "          pair_to_length_dict[x, y] = l\n",
    "    if max(pair_to_length_dict.values()) < min_length:\n",
    "      raise ValueError(\"All shortest paths are below the minimum length\")\n",
    "    # The node pairs which exceed the minimum length.\n",
    "    node_pairs = list(pair_to_length_dict)\n",
    "\n",
    "    # Computes probabilities per pair, to enforce uniform sampling of each\n",
    "    # shortest path lengths.\n",
    "    # The counts of pairs per length.\n",
    "    counts = collections.Counter(pair_to_length_dict.values())\n",
    "    prob_per_length = 1.0 / len(counts)\n",
    "    probabilities = [\n",
    "        prob_per_length / counts[pair_to_length_dict[x]]\n",
    "        for x in node_pairs\n",
    "    ]\n",
    "\n",
    "    # Choose the start and end points.\n",
    "    i = rand.choice(len(node_pairs), p=probabilities)\n",
    "    start, end = node_pairs[i]\n",
    "    path = nx.shortest_path(graph,\n",
    "                            source=start,\n",
    "                            target=end,\n",
    "                            weight=DISTANCE_WEIGHT_NAME)\n",
    "\n",
    "    # Creates a directed graph, to store directed path from start to end.\n",
    "    digraph = graph.to_directed()\n",
    "\n",
    "    # Add \"start\", \"end\", and \"solution\" attributes to the nodes and edges.\n",
    "    digraph.add_node(start, start=True)\n",
    "    digraph.add_node(end, end=True)\n",
    "    digraph.add_nodes_from(set_diff(digraph.nodes(), [start]), start=False)\n",
    "    digraph.add_nodes_from(set_diff(digraph.nodes(), [end]), end=False)\n",
    "    digraph.add_nodes_from(set_diff(digraph.nodes(), path), solution=False)\n",
    "    digraph.add_nodes_from(path, solution=True)\n",
    "    path_edges = list(pairwise(path))\n",
    "    digraph.add_edges_from(set_diff(digraph.edges(), path_edges),\n",
    "                           solution=False)\n",
    "    digraph.add_edges_from(path_edges, solution=True)\n",
    "\n",
    "    return digraph\n",
    "\n",
    "  def graph_to_input_target(self, graph):\n",
    "    \"\"\"Returns 2 graphs with input and target feature vectors for training.\n",
    "\n",
    "    Args:\n",
    "      graph: An `nx.DiGraph` instance.\n",
    "\n",
    "    Returns:\n",
    "      The input `nx.DiGraph` instance.\n",
    "      The target `nx.DiGraph` instance.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: unknown node type\n",
    "    \"\"\"\n",
    "\n",
    "    def create_feature(attr, fields):\n",
    "      return np.hstack([np.array(attr[field], dtype=float) for field in fields])\n",
    "\n",
    "    input_node_fields = (\"pos\", \"weight\", \"start\", \"end\")\n",
    "    input_edge_fields = (\"distance\", )\n",
    "    target_node_fields = (\"solution\", )\n",
    "    target_edge_fields = (\"solution\", )\n",
    "\n",
    "    input_graph = graph.copy()\n",
    "    target_graph = graph.copy()\n",
    "\n",
    "    solution_length = 0\n",
    "    for node_index, node_feature in graph.nodes(data=True):\n",
    "      input_graph.add_node(node_index,\n",
    "                           features=create_feature(\n",
    "                               node_feature, input_node_fields))\n",
    "      target_node = to_one_hot(\n",
    "          create_feature(node_feature, target_node_fields).astype(int),\n",
    "          2)[0]\n",
    "      target_graph.add_node(node_index, features=target_node)\n",
    "      solution_length += int(node_feature[\"solution\"])\n",
    "    solution_length /= graph.number_of_nodes()\n",
    "\n",
    "    for receiver, sender, features in graph.edges(data=True):\n",
    "      input_graph.add_edge(sender,\n",
    "                           receiver,\n",
    "                           features=create_feature(\n",
    "                               features, input_edge_fields))\n",
    "      target_edge = to_one_hot(\n",
    "          create_feature(features, target_edge_fields).astype(int), 2)[0]\n",
    "      target_graph.add_edge(sender, receiver, features=target_edge)\n",
    "\n",
    "    input_graph.graph[\"features\"] = np.array([0.0])\n",
    "    target_graph.graph[\"features\"] = np.array([solution_length],\n",
    "                                              dtype=float)\n",
    "\n",
    "    return input_graph, target_graph\n",
    "\n",
    "  def gen_sample(self, name, index):\n",
    "    seed = abs(hash(name)*index) % (2**32)\n",
    "    theta = 1001.0\n",
    "    rate = 1.1\n",
    "    rand = np.random.RandomState(seed=seed)  # TODO: Figure out seed\n",
    "    graph = self.generate_graph(rand, (self.min_nodes, self.max_nodes),\n",
    "                                theta=theta)[0]\n",
    "    graph = self.add_shortest_path(rand, graph)\n",
    "    input_graph, target_graph = self.graph_to_input_target(graph)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    input_graph_dict = utils_np.networkx_to_data_dict(input_graph)\n",
    "    target_graph_dict = utils_np.networkx_to_data_dict(target_graph)\n",
    "    adj_mat_dense = np.zeros((self.max_nodes, self.max_nodes))\n",
    "    n_node = input_graph_dict['n_node']\n",
    "    adj_mat_dense[:n_node, :n_node] = nx.to_numpy_matrix(input_graph)\n",
    "    adj_mat_sparse = data_util.np_dense_to_sparse(adj_mat_dense)\n",
    "    return {\n",
    "        'input_graph': input_graph_dict,\n",
    "        'target_graph': target_graph_dict,\n",
    "        'adj_mat_dense': adj_mat_dense,\n",
    "        'adj_mat_sparse': adj_mat_sparse,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:00<00:01, 55.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Pose Graphs\n",
      "Writing dataset to testgraphs/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 63.94it/s]\n",
      "  6%|▌         | 6/100 [00:00<00:01, 58.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset to testgraphs/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 63.40it/s]\n",
      "  6%|▌         | 6/100 [00:00<00:01, 55.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset to testgraphs/np_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.49it/s]\n",
      "  5%|▌         | 5/100 [00:00<00:01, 47.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset to testgraphs/np_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# GENERATE DATASET\n",
    "print(\"Generating Pose Graphs\")\n",
    "testgraphs_data_dir = 'testgraphs'\n",
    "if not os.path.exists(testgraphs_data_dir):\n",
    "  os.makedirs(testgraphs_data_dir)\n",
    "mydataset = ShortestPathGraphDataset(testgraphs_data_dir, 30, 40)\n",
    "\n",
    "types = [('train', 100), ('test', 100)]\n",
    "for t, n in types:\n",
    "  dname = os.path.join(testgraphs_data_dir, t)\n",
    "  if not os.path.exists(dname):\n",
    "    os.makedirs(dname)\n",
    "  mydataset.convert_dataset(t, n)\n",
    "\n",
    "# Generate numpy test\n",
    "mydataset.create_np_dataset('np_train', 100)\n",
    "mydataset.create_np_dataset('np_test', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(target, output, use_nodes=True, use_edges=False):\n",
    "  \"\"\"Calculate model accuracy.\n",
    "\n",
    "  Returns the number of correctly predicted shortest path nodes and the number\n",
    "  of completely solved graphs (100% correct predictions).\n",
    "\n",
    "  Args:\n",
    "    target: A `graphs.GraphsTuple` that contains the target graph.\n",
    "    output: A `graphs.GraphsTuple` that contains the output graph.\n",
    "    use_nodes: A `bool` indicator of whether to compute node accuracy or not.\n",
    "    use_edges: A `bool` indicator of whether to compute edge accuracy or not.\n",
    "\n",
    "  Returns:\n",
    "    correct: A `float` fraction of correctly labeled nodes/edges.\n",
    "    solved: A `float` fraction of graphs that are completely correctly\n",
    "      labeled.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: Nodes or edges (or both) must be used\n",
    "  \"\"\"\n",
    "  if not use_nodes and not use_edges:\n",
    "      raise ValueError(\"Nodes or edges (or both) must be used\")\n",
    "  tdds = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "  odds = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "  cs = []\n",
    "  ss = []\n",
    "  for td, od in zip(tdds, odds):\n",
    "    xn = np.argmax(td[\"nodes\"], axis=-1)\n",
    "    yn = np.argmax(od[\"nodes\"], axis=-1)\n",
    "    xe = np.argmax(td[\"edges\"], axis=-1)\n",
    "    ye = np.argmax(od[\"edges\"], axis=-1)\n",
    "    c = []\n",
    "    if use_nodes:\n",
    "      c.append(xn == yn)\n",
    "    if use_edges:\n",
    "      c.append(xe == ye)\n",
    "    c = np.concatenate(c, axis=0)\n",
    "    s = np.all(c)\n",
    "    cs.append(c)\n",
    "    ss.append(s)\n",
    "  correct = np.mean(np.concatenate(cs, axis=0))\n",
    "  solved = np.mean(np.stack(ss))\n",
    "  return correct, solved\n",
    "\n",
    "\n",
    "def create_loss_ops(target_op, output_ops):\n",
    "  loss_ops = [\n",
    "      tf.losses.softmax_cross_entropy(target_op.nodes, output_op.nodes) +\n",
    "      tf.losses.softmax_cross_entropy(target_op.edges, output_op.edges)\n",
    "      for output_op in output_ops\n",
    "  ]\n",
    "  return loss_ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_none(x):\n",
    "  return None\n",
    "\n",
    "def run_training(input_ph,\n",
    "                 target_ph,\n",
    "                 get_vals=ret_none,\n",
    "                 get_feed_dict=ret_none):\n",
    "  # Connect the data to the model.\n",
    "  # Instantiate the model.\n",
    "  model = models.EncodeProcessDecode(edge_output_size=2, node_output_size=2)\n",
    "  # A list of outputs, one per processing step.\n",
    "  num_processing_steps = 10\n",
    "  output_ops_tr = model(input_ph, num_processing_steps)\n",
    "  output_ops_ge = model(input_ph, num_processing_steps)\n",
    "  # Training loss.\n",
    "  loss_ops_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "  # Loss across processing steps.\n",
    "  loss_op_tr = sum(loss_ops_tr) / num_processing_steps\n",
    "  # Test/generalization loss.\n",
    "  loss_ops_ge = create_loss_ops(target_ph, output_ops_ge)\n",
    "  loss_op_ge = loss_ops_ge[-1]  # Loss from final processing step.\n",
    "\n",
    "  # Optimizer.\n",
    "  learning_rate = 1e-3\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "  step_op = optimizer.minimize(loss_op_tr)\n",
    "\n",
    "  try:\n",
    "      sess.close()\n",
    "  except NameError:\n",
    "      pass\n",
    "  sess = tf.Session()\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  last_iteration = 0\n",
    "  logged_iterations = []\n",
    "  losses_tr = []\n",
    "  corrects_tr = []\n",
    "  solveds_tr = []\n",
    "  losses_ge = []\n",
    "  corrects_ge = []\n",
    "  solveds_ge = []\n",
    "\n",
    "  # How much time between logging and printing the current results.\n",
    "  num_training_iterations = 1000\n",
    "  log_every_seconds = 2\n",
    "\n",
    "  print(\"# (iteration number), T (elapsed seconds), \"\n",
    "        \"Ltr (training loss), Lge (test/generalization loss), \"\n",
    "        \"Ctr (training fraction nodes/edges labeled correctly), \"\n",
    "        \"Str (training fraction examples solved correctly), \"\n",
    "        \"Cge (test/generalization fraction nodes/edges labeled correctly), \"\n",
    "        \"Sge (test/generalization fraction examples solved correctly)\")\n",
    "\n",
    "  start_time = time.time()\n",
    "  last_log_time = start_time\n",
    "  for iteration in range(last_iteration, num_training_iterations):\n",
    "    last_iteration = iteration\n",
    "    vals = get_vals(iteration)\n",
    "    train_values = sess.run({\n",
    "        \"step\": step_op,\n",
    "        \"target\": target_ph,\n",
    "        \"loss\": loss_op_tr,\n",
    "        \"outputs\": output_ops_tr\n",
    "    }, feed_dict=get_feed_dict(vals))\n",
    "    the_time = time.time()\n",
    "    elapsed_since_last_log = the_time - last_log_time\n",
    "    if elapsed_since_last_log > log_every_seconds:\n",
    "      last_log_time = the_time\n",
    "      test_values = sess.run({\n",
    "          \"target\": target_ph,\n",
    "          \"loss\": loss_op_ge,\n",
    "          \"outputs\": output_ops_ge\n",
    "      }, feed_dict=get_feed_dict(vals))\n",
    "      correct_tr, solved_tr = compute_accuracy(train_values[\"target\"],\n",
    "                                               train_values[\"outputs\"][-1],\n",
    "                                               use_edges=True)\n",
    "      correct_ge, solved_ge = compute_accuracy(test_values[\"target\"],\n",
    "                                               test_values[\"outputs\"][-1],\n",
    "                                               use_edges=True)\n",
    "      elapsed = time.time() - start_time\n",
    "      losses_tr.append(train_values[\"loss\"])\n",
    "      corrects_tr.append(correct_tr)\n",
    "      solveds_tr.append(solved_tr)\n",
    "      losses_ge.append(test_values[\"loss\"])\n",
    "      corrects_ge.append(correct_ge)\n",
    "      solveds_ge.append(solved_ge)\n",
    "      logged_iterations.append(iteration)\n",
    "      print(\"# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Ctr {:.4f}, Str\"\n",
    "            \" {:.4f}, Cge {:.4f}, Sge {:.4f}\".format(iteration, elapsed,\n",
    "                                                     train_values[\"loss\"],\n",
    "                                                     test_values[\"loss\"],\n",
    "                                                     correct_tr, solved_tr,\n",
    "                                                     correct_ge, solved_ge))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephen/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py:1662: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################\n",
      "Using TFRecords....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephen/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# (iteration number), T (elapsed seconds), Ltr (training loss), Lge (test/generalization loss), Ctr (training fraction nodes/edges labeled correctly), Str (training fraction examples solved correctly), Cge (test/generalization fraction nodes/edges labeled correctly), Sge (test/generalization fraction examples solved correctly)\n",
      "# 00000, T 9.2, Ltr 2.6663, Lge 2.1821, Ctr 0.4958, Str 0.0000, Cge 0.5713, Sge 0.0000\n",
      "# 00021, T 10.1, Ltr 1.0943, Lge 1.0865, Ctr 0.7840, Str 0.0000, Cge 0.7794, Sge 0.0000\n",
      "# 00067, T 12.2, Ltr 0.9338, Lge 0.9289, Ctr 0.8147, Str 0.0000, Cge 0.8131, Sge 0.0000\n",
      "# 00112, T 14.2, Ltr 0.9161, Lge 0.7723, Ctr 0.8155, Str 0.0000, Cge 0.8659, Sge 0.0000\n",
      "# 00158, T 16.2, Ltr 0.9370, Lge 0.8810, Ctr 0.7917, Str 0.0000, Cge 0.8062, Sge 0.0000\n",
      "# 00204, T 18.3, Ltr 0.7571, Lge 0.7763, Ctr 0.8652, Str 0.0000, Cge 0.8611, Sge 0.0000\n",
      "# 00251, T 20.3, Ltr 0.9121, Lge 0.9017, Ctr 0.7981, Str 0.0000, Cge 0.7999, Sge 0.0000\n",
      "# 00298, T 22.3, Ltr 0.8320, Lge 0.7549, Ctr 0.8094, Str 0.0000, Cge 0.8462, Sge 0.0000\n",
      "# 00345, T 24.3, Ltr 0.6652, Lge 0.8361, Ctr 0.8612, Str 0.0000, Cge 0.8241, Sge 0.0000\n",
      "# 00392, T 26.3, Ltr 0.7257, Lge 0.7079, Ctr 0.8300, Str 0.0000, Cge 0.8339, Sge 0.0000\n",
      "# 00439, T 28.3, Ltr 0.6531, Lge 0.5621, Ctr 0.8782, Str 0.0000, Cge 0.8630, Sge 0.0000\n",
      "# 00483, T 30.4, Ltr 0.5447, Lge 0.3661, Ctr 0.9163, Str 0.0000, Cge 0.9094, Sge 0.0625\n",
      "# 00526, T 32.4, Ltr 0.7231, Lge 0.7503, Ctr 0.8524, Str 0.0000, Cge 0.8393, Sge 0.0000\n",
      "# 00571, T 34.4, Ltr 0.5882, Lge 0.4651, Ctr 0.9047, Str 0.0000, Cge 0.8878, Sge 0.0000\n",
      "# 00618, T 36.4, Ltr 0.4870, Lge 0.5299, Ctr 0.9301, Str 0.0000, Cge 0.8689, Sge 0.0000\n",
      "# 00665, T 38.4, Ltr 0.5711, Lge 0.3179, Ctr 0.8963, Str 0.0000, Cge 0.9031, Sge 0.0000\n",
      "# 00712, T 40.4, Ltr 0.5141, Lge 0.5640, Ctr 0.9102, Str 0.0000, Cge 0.8692, Sge 0.0000\n",
      "# 00759, T 42.4, Ltr 0.5319, Lge 0.2965, Ctr 0.9015, Str 0.0000, Cge 0.9152, Sge 0.0000\n",
      "# 00806, T 44.4, Ltr 0.6171, Lge 0.3131, Ctr 0.8810, Str 0.0625, Cge 0.9129, Sge 0.0000\n",
      "# 00846, T 46.4, Ltr 0.4712, Lge 0.3217, Ctr 0.9168, Str 0.0000, Cge 0.8976, Sge 0.0000\n",
      "# 00891, T 48.5, Ltr 0.5443, Lge 0.4067, Ctr 0.8906, Str 0.0000, Cge 0.8729, Sge 0.0000\n",
      "# 00935, T 50.5, Ltr 0.4356, Lge 0.2168, Ctr 0.9253, Str 0.0000, Cge 0.9397, Sge 0.0000\n",
      "# 00980, T 52.5, Ltr 0.5059, Lge 0.3868, Ctr 0.9080, Str 0.0000, Cge 0.8938, Sge 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Create tensors\n",
    "print('#####################################################')\n",
    "print(\"Using TFRecords....\")\n",
    "sample = mydataset.load_batch('train', 16)\n",
    "input_ph = sample['input_graph']\n",
    "target_ph = sample['target_graph']\n",
    "run_training(input_ph,\n",
    "             target_ph,\n",
    "             get_vals=ret_none,\n",
    "             get_feed_dict=ret_none)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################\n",
      "Using Placeholders....\n",
      "# (iteration number), T (elapsed seconds), Ltr (training loss), Lge (test/generalization loss), Ctr (training fraction nodes/edges labeled correctly), Str (training fraction examples solved correctly), Cge (test/generalization fraction nodes/edges labeled correctly), Sge (test/generalization fraction examples solved correctly)\n",
      "# 00000, T 9.9, Ltr 0.9613, Lge 0.7695, Ctr 0.8447, Str 0.0000, Cge 0.8932, Sge 0.0000\n",
      "# 00050, T 10.7, Ltr 0.7273, Lge 0.7298, Ctr 0.9340, Str 0.0000, Cge 0.9340, Sge 0.0000\n",
      "# 00172, T 12.7, Ltr 1.2060, Lge 1.1889, Ctr 0.7010, Str 0.0000, Cge 0.6907, Sge 0.0000\n",
      "# 00290, T 14.8, Ltr 0.6888, Lge 0.6292, Ctr 0.9072, Str 0.0000, Cge 0.9175, Sge 0.0000\n",
      "# 00409, T 16.8, Ltr 0.9671, Lge 0.9434, Ctr 0.7500, Str 0.0000, Cge 0.7500, Sge 0.0000\n",
      "# 00528, T 18.8, Ltr 0.9464, Lge 0.9450, Ctr 0.8191, Str 0.0000, Cge 0.8191, Sge 0.0000\n",
      "# 00647, T 20.8, Ltr 0.8606, Lge 0.8613, Ctr 0.8214, Str 0.0000, Cge 0.8304, Sge 0.0000\n",
      "# 00767, T 22.8, Ltr 0.3409, Lge 0.2853, Ctr 0.9432, Str 0.0000, Cge 0.9432, Sge 0.0000\n",
      "# 00887, T 24.8, Ltr 0.8928, Lge 0.9016, Ctr 0.8681, Str 0.0000, Cge 0.8681, Sge 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Create tensors\n",
    "print('#####################################################')\n",
    "print(\"Using Placeholders....\")\n",
    "sample, placeholders = mydataset.get_placeholders(True)\n",
    "input_ph = sample['input_graph']\n",
    "target_ph = sample['target_graph']\n",
    "\n",
    "def get_vals(iteration):\n",
    "  index = iteration % mydataset.sizes['np_train']\n",
    "  return mydataset.load_npz_file('np_train', index)\n",
    "\n",
    "def get_feed_dict(vals):\n",
    "  fdict = mydataset.get_feed_dict(placeholders, vals, True)\n",
    "  return fdict\n",
    "\n",
    "run_training(input_ph,\n",
    "             target_ph,\n",
    "             get_vals=get_vals,\n",
    "             get_feed_dict=get_feed_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
